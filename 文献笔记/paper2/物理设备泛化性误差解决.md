### 核心问题：光学系统实际采样的图像与我们仿真出来的图像具有较大的误差，如果我们想让光学硬件部署到应用中，必须解决的是让真实的光学数据在测试的时候能够与仿真有差距不大的效果。(通常的解决方案是微调)
	
#### 这个问题可以视为：
	仿真层是训练结果，实际的光学结果是测试结果。这样的话，我们的目标就是提高网络的泛化性。
	我们希望网络能够不受光学噪声以及各种误差包括系统误差以及仿真和实际系统的差距等因素的影响，注意这里我们的思路是倾向于去噪。(思考这里和虚拟误差以及微调的区别)	

### 法1：虚假相关性角度解决
	目标：从仿真数据和实际数据的分布差异角度切入(训练集和测试集的分布差异)
	      对来自许多环境的数据进行训练并找到不变预测变量，通过将模型集中在与结          果有因果关系的特征上来减少虚假特征的影响

### 1.它能不能算是一个虚假相关性问题？或者说如何把它变成一个虚假相关性问题
  
	  虚假相关性：输入的非基本特征（例如，背景、纹理和辅助对象）与相应标签之间高度依赖虚假相关性
	  不是为了让某一个特定环境变得更好，而是为了找到一个统一的、稳健的 (robust) 模型，这个模型能够学到所有环境背后共同的、不变的规律（即因果关系） ，而忽略掉每个环境特有的“伪关联”
  
  ==注意这里和去噪的区别和共同点，前者是为了在两个环境的数据集中学习到共同的特征，后者是以无噪声环境监督下对数据去噪==




虚假相关性：指一种不能确定的隐藏的分布关系，但是噪声的影响是以知的，也就是说，拥有虚假相关性问题的数据集往往是需要在不同的相关性之间权衡出一种共同的特征，但是关于我们的目标在于实际效果接近于仿真效果。



问题就在于：
	1.仿真的图像具有的“信号的绝对完美性”本身是一种虚假特征，会学到一个极其危险的快捷方式：“只有那些边缘锐利、信噪比无限高、完全符合理想物理模型的信号，才是有效的、值得信任的信号.
	2.真实的图像会将带有噪声的信号作为一种虚假特征

因此问题就变成了：
如何训练一个模型，让它同时忽略掉“仿真世界的完美性”和“真实世界的特定噪声模式”这两种虚假特征，而只专注于那个在两个世界中都稳定存在的、真正的“因果特征”——即场景本身的物理信息。

紧接着：现有方法都需要对数据集进行手动划分就是对于不同环境进行标签划分，我们面临一个问题，与有明确环境差异不同(比如明确的背景信息)，光学真实图像背景误差复杂，各种误差(像差，对齐误差等等各种误差云集，零级衍射等等)，难以对图像划分label.
2.对于数据集比较挑剔 只对一些简单的线性任务起作用

#### **不变预测器 (Invariant predictor)**：这是一个核心概念 。一个“不变预测器”由两部分组成：一个数据表示函数

Φ 和一个分类器 _w_ 。如果存在一个分类器 _w_，在数据经过表示函数 Φ 变换后，能够**同时在所有环境中都达到最小的风险**，那么我们就说这个数据表示 Φ 引出了一个不变预测器 w∘Φ 。



#### 但是这种思路可能是可行的





### 解决思路2：DFR fine-tuning

虚假相关性：就是说有一个虚假的相关性让网络依赖，如果无噪声的仿真图象，完美的信号背景会给网络一个依赖从而产生一个虚假相关性，那么真实的图像就是打破了这样一种虚假相关性，效果势必会差。但是关键在于我们有没有保留完整的特征？

如果使用虚假相关性解决问题：问题就变成了在真实的噪声数据集上的表现差的原因是因为仿真图像的“无噪声”构成了一种虚假相关性
